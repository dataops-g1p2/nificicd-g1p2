# NiFi CI/CD Project - Tools & Environment Overview

## üìã Project Context

**Problem Statement**: Data pipelines are currently deployed manually across NiFi environments (Development, Test, Production). This process is time-consuming and error-prone.

**Objective**: Implement a CI/CD pipeline to automate the NiFi flow lifecycle management, reduce human errors, save time, and standardize environments.

---

## üèóÔ∏è Three-Environment Architecture


![Environment Architecture Diagram](docs/assets/environnement_diagram2.png)



## üõ†Ô∏è Tools by Environment

### **1. Development Environment (DEV)**

| Component | Specification | Purpose | Why It Matters |
|-----------|--------------|---------|----------------|
| **NiFi Standalone** | ‚Ä¢ 1 Instance<br>‚Ä¢ 4-8 GB RAM<br>‚Ä¢ 2-4 CPU<br>‚Ä¢ Port: 8080 (HTTP) | Flow development and unit testing workspace | **Cost-effective**: Single node sufficient for development<br>**Flexibility**: Developers can iterate quickly without cluster complexity |
| **NiFi Registry** | ‚Ä¢ Shared or dedicated<br>‚Ä¢ Port: 18080<br>‚Ä¢ Git backend | Version control for flows | **Version Control**: Every flow change tracked in Git<br>**Collaboration**: Multiple developers can work on different flows<br>**Audit Trail**: Complete history of who changed what and when |
| **PostgreSQL/MySQL** | ‚Ä¢ Single instance<br>‚Ä¢ Port: 5432/3306<br>‚Ä¢ Test/sample data | Development database | **Local Testing**: Developers can test database interactions<br>**Data Isolation**: Development data separate from production |
| **GitHub** | ‚Ä¢ Repository<br>‚Ä¢ Webhooks configured | Source code management | **Single Source of Truth**: All code and flows stored centrally<br>**CI/CD Trigger**: Automated pipeline on code push |

**Resource Allocation**: 4-8 GB RAM, 2-4 CPU per instance  
**Security**: Basic authentication (single-user or LDAP)  
**Backup**: Not critical (development data)

---

### **2. Test/UAT Environment (TEST)**

| Component | Specification | Purpose | Why It Matters |
|-----------|--------------|---------|----------------|
| **NiFi Cluster** | ‚Ä¢ 2-3 Nodes<br>‚Ä¢ 8-16 GB RAM each<br>‚Ä¢ Port: 8080/8443<br>‚Ä¢ Clustered mode | Integration testing and validation | **Realistic Testing**: Mimics production cluster behavior<br>**Load Testing**: Can validate performance under load<br>**Failover Testing**: Test high availability scenarios |
| **ZooKeeper Cluster** | ‚Ä¢ 3 Nodes<br>‚Ä¢ Ports: 2181, 2888, 3888<br>‚Ä¢ Quorum-based | Cluster coordination and leader election | **Cluster Management**: Coordinates NiFi nodes<br>**State Management**: Maintains cluster consistency<br>**Configuration Management**: Distributes config changes |
| **NiFi Registry** | ‚Ä¢ Shared with Prod or dedicated<br>‚Ä¢ Port: 18080 | Version control (shared or isolated) | **Flow Promotion**: Flows tested here before production<br>**Rollback Capability**: Can revert to previous versions |
| **PostgreSQL/MySQL** | ‚Ä¢ Clone or subset of prod<br>‚Ä¢ Port: 5432/3306<br>‚Ä¢ Realistic data volume | Test database with production-like data | **Realistic Testing**: Tests with production data patterns<br>**Performance Validation**: Identifies bottlenecks before prod |
| **Monitoring Stack** (Optional) | ‚Ä¢ Prometheus<br>‚Ä¢ Grafana<br>‚Ä¢ ELK Stack | Performance monitoring and log analysis | **Early Detection**: Identify issues before production<br>**Performance Baseline**: Establish expected metrics |

**Resource Allocation**: 8-16 GB RAM per node  
**Security**: Moderate (SSL optional, basic auth)  
**Backup**: Daily backups recommended  
**Purpose**: Integration tests, UAT, performance testing

---

### **3. Production Environment (PROD)**

| Component | Specification | Purpose | Why It Matters |
|-----------|--------------|---------|----------------|
| **NiFi Cluster** | ‚Ä¢ 3+ Nodes (HA)<br>‚Ä¢ 16-32 GB RAM each<br>‚Ä¢ Port: 8443 (HTTPS only)<br>‚Ä¢ SSL/TLS enabled | Live data processing with high availability | **Zero Downtime**: Node failures don't stop operations<br>**Scalability**: Add nodes to handle increased load<br>**Performance**: Distributed processing of high-volume data |
| **ZooKeeper Cluster** | ‚Ä¢ 3-5 Nodes<br>‚Ä¢ Ports: 2181, 2888, 3888<br>‚Ä¢ Production-grade quorum | Critical cluster coordination | **Reliability**: 5-node setup survives 2 node failures<br>**Split-Brain Prevention**: Quorum ensures consistency |
| **NiFi Registry** | ‚Ä¢ Dedicated HA setup<br>‚Ä¢ Port: 18443 (HTTPS)<br>‚Ä¢ High availability | Production flow versioning | **Business Continuity**: Critical component must be HA<br>**Disaster Recovery**: Can restore flows from registry |
| **Load Balancer** | ‚Ä¢ nginx or HAProxy<br>‚Ä¢ Port: 443<br>‚Ä¢ SSL termination<br>‚Ä¢ Health checks | Traffic distribution and failover | **Single Entry Point**: Simplifies client configuration<br>**Automatic Failover**: Routes traffic away from failed nodes<br>**SSL Offload**: Centralizes certificate management |
| **PostgreSQL Cluster** | ‚Ä¢ Primary + Replicas<br>‚Ä¢ Port: 5432<br>‚Ä¢ Replication enabled<br>‚Ä¢ Auto-failover | Production database with HA | **Data Durability**: Replicas protect against data loss<br>**Read Scaling**: Replicas handle read queries<br>**Automatic Recovery**: Failover without manual intervention |
| **Prometheus** | ‚Ä¢ High availability setup<br>‚Ä¢ Scrape interval: 15s<br>‚Ä¢ Retention: 30 days | Metrics collection and alerting | **Real-time Monitoring**: Immediate visibility into issues<br>**Trend Analysis**: Historical data for capacity planning<br>**Alerting**: Proactive notification of problems |
| **Grafana** | ‚Ä¢ Dashboards<br>‚Ä¢ Multiple data sources<br>‚Ä¢ Role-based access | Visualization and operational dashboards | **Operations Visibility**: Teams see system health at a glance<br>**Executive Reporting**: Business metrics visualization |
| **ELK Stack** | ‚Ä¢ Elasticsearch<br>‚Ä¢ Logstash<br>‚Ä¢ Kibana<br>‚Ä¢ Centralized logging | Log aggregation and analysis | **Troubleshooting**: Quickly find root cause of issues<br>**Audit Trail**: Compliance and security logging<br>**Correlation**: Connect logs across all components |
| **HashiCorp Vault** | ‚Ä¢ Port: 8200<br>‚Ä¢ HA setup<br>‚Ä¢ Dynamic secrets | Secure secrets management | **Security**: No hardcoded passwords in code<br>**Rotation**: Automatic credential rotation<br>**Audit**: Complete access logs for compliance |
| **Backup System** | ‚Ä¢ Automated daily backups<br>‚Ä¢ Off-site storage<br>‚Ä¢ Restore procedures | Disaster recovery | **Business Continuity**: Recover from catastrophic failure<br>**Compliance**: Meet data retention requirements |
| **PagerDuty** | ‚Ä¢ On-call rotation<br>‚Ä¢ Escalation policies<br>‚Ä¢ Integration with alerts | Incident management | **Response Time**: Critical issues reach right person quickly<br>**Accountability**: Clear ownership of incidents |

**Resource Allocation**: 16-32 GB RAM per node  
**Security**: Maximum (HTTPS, mTLS, RBAC, secrets management)  
**Backup**: Real-time replication + daily snapshots  
**SLA**: 99.9% uptime target

---

## üîÑ CI/CD Pipeline Tools

### **GitLab CI / GitHub Actions**
**Purpose**: Automate deployment lifecycle

**Pipeline Stages**:
1. **Validate** - Syntax checking, standards compliance
2. **Test** - Unit and integration tests (pytest)
3. **Deploy-Dev** - Automatic deployment after code push
4. **Deploy-Test** - Automatic deployment after successful tests
5. **Deploy-Prod** - Manual approval gate + deployment

**Why Critical**: 
- Eliminates manual errors
- Reduces deployment time from hours to minutes
- Ensures consistency across environments
- Provides rollback capability

### **Python Deployment Scripts**
**Purpose**: Interact with NiFi REST API for automated deployments

**Key Scripts**:
- `deploy_nifi.py` - Main deployment orchestration
- `apply_parameters.py` - Environment-specific configuration
- `validate_flow.py` - Pre-deployment validation
- `backup_prod.py` - Pre-deployment backup

**Why Important**:
- Custom automation not available out-of-box
- Handles complex deployment logic
- Integrates with all other tools

### **Docker** (Optional)
**Purpose**: Containerization for portable deployments

**Usage**:
- Development environment consistency
- CI/CD pipeline execution
- Test environment isolation

---

## üìä Comparison Table: Tools by Environment

| Tool/Component | DEV | TEST | PROD | Purpose |
|----------------|-----|------|------|---------|
| **NiFi** | Standalone (1 node) | Cluster (2-3 nodes) | Cluster (3+ nodes) | Data flow platform |
| **NiFi Registry** | Basic | Shared/Dedicated | Dedicated HA | Flow versioning |
| **ZooKeeper** | Not needed | 3 nodes | 3-5 nodes | Cluster coordination |
| **Database** | Single instance | Clone of prod | HA Cluster | Data persistence |
| **Load Balancer** | No | Optional | Required (nginx) | Traffic distribution |
| **Monitoring** | No | Optional | Required (Prom+Graf) | Observability |
| **Logging** | Local logs | Optional (ELK) | Required (ELK) | Troubleshooting |
| **Secrets Mgmt** | Plaintext config | Env variables | Vault | Security |
| **Backup** | No | Daily | Real-time + Daily | Disaster recovery |
| **SSL/TLS** | No | Optional | Required | Security |
| **Resources/Node** | 4-8 GB RAM | 8-16 GB RAM | 16-32 GB RAM | Performance |

---

## üí∞ Cost vs. Capability Analysis

### **Development Environment**
- **Cost**: $ (Low)
- **Purpose**: Developer productivity
- **Uptime SLA**: None (business hours only)
- **Acceptable Downtime**: Hours to days

### **Test Environment**
- **Cost**: $$ (Medium)
- **Purpose**: Quality assurance, avoid prod issues
- **Uptime SLA**: 90% (best effort)
- **Acceptable Downtime**: Hours

### **Production Environment**
- **Cost**: $$$$ (High)
- **Purpose**: Business operations
- **Uptime SLA**: 99.9% (8.76 hours/year downtime)
- **Acceptable Downtime**: Minutes

**Investment Justification**:
- **Time Savings**: 90% reduction in deployment time
- **Error Reduction**: 85% fewer deployment issues
- **Business Continuity**: Production HA prevents costly outages

---

## üîê Security Considerations by Environment

### **Development**
- ‚úÖ Basic authentication
- ‚úÖ HTTP allowed
- ‚úÖ Local file storage
- ‚ùå No encryption at rest
- ‚ùå No audit logging

### **Test**
- ‚úÖ LDAP/SSO integration
- ‚úÖ HTTPS optional
- ‚úÖ Role-based access
- ‚úÖ Basic audit logging
- ‚ùå Secrets in environment variables

### **Production**
- ‚úÖ HTTPS only (TLS 1.3)
- ‚úÖ Mutual TLS (mTLS)
- ‚úÖ LDAP/SSO with MFA
- ‚úÖ Granular RBAC
- ‚úÖ HashiCorp Vault for secrets
- ‚úÖ Encryption at rest
- ‚úÖ Complete audit logging
- ‚úÖ Security scanning
- ‚úÖ Network segmentation

---

## üìà Expected Benefits

### **Quantifiable Improvements**

| Metric | Before CI/CD | After CI/CD | Improvement |
|--------|-------------|-------------|-------------|
| **Deployment Time** | 4-6 hours | 15-30 minutes | 90% reduction |
| **Deployment Errors** | 20-30% | <5% | 85% reduction |
| **Environment Drift** | Frequent | Rare | Consistency achieved |
| **Rollback Time** | Hours-Days | Minutes | 95% reduction |
| **Audit Trail** | Manual docs | Automatic (Git) | 100% coverage |
| **Test Coverage** | Manual/Ad-hoc | Automated | Consistent |

### **Qualitative Benefits**
- ‚úÖ **Developer Productivity**: Focus on flow logic, not deployment mechanics
- ‚úÖ **Operations Confidence**: "Works in test = works in prod"
- ‚úÖ **Compliance**: Complete audit trail for regulatory requirements
- ‚úÖ **Disaster Recovery**: Rapid restoration from Registry + Git
- ‚úÖ **Knowledge Sharing**: Infrastructure documented as code

---

## üöÄ Getting Started

### **Phase 1: Infrastructure Setup (Weeks 1-2)**
1. Provision servers/VMs for all three environments
2. Install NiFi on each environment
3. Install and configure NiFi Registry
4. Set up ZooKeeper clusters (Test & Prod)
5. Configure databases
6. Set up networking and firewalls

### **Phase 2: Configuration (Weeks 3-4)**
1. Create Parameter Contexts for each environment
2. Configure NiFi Registry ‚Üí GitHub integration
3. Create Registry buckets (dev-flows, test-flows, prod-flows)
4. Set up authentication (LDAP for prod)
5. Deploy HashiCorp Vault (prod)

### **Phase 3: CI/CD Pipeline (Weeks 5-6)**
1. Create GitHub repository structure
2. Develop Python deployment scripts
3. Configure GitLab CI/CD pipeline
4. Create automated tests
5. Test full deployment flow: Dev ‚Üí Test ‚Üí Prod
6. Document rollback procedures

### **Phase 4: Monitoring (Week 7)**
1. Deploy Prometheus + Grafana
2. Configure alerting rules
3. Set up ELK stack for centralized logging
4. Create operational dashboards
5. Configure PagerDuty integration

### **Phase 5: Training & Handoff (Week 8)**
1. Document all procedures
2. Create runbooks for common scenarios
3. Train development and operations teams
4. Conduct disaster recovery drill
5. Go-live validation

---

## üìö Documentation Deliverables

1. **Architecture Diagram** (this document)
2. **Deployment Guide** - Step-by-step deployment instructions
3. **Runbook** - Operational procedures for common tasks
4. **Disaster Recovery Plan** - Restoration procedures
5. **Troubleshooting Guide** - Common issues and solutions
6. **API Documentation** - NiFi REST API usage examples

---

## ‚úÖ Success Criteria

- [ ] All three environments operational
- [ ] CI/CD pipeline successfully deploys from Dev ‚Üí Test ‚Üí Prod
- [ ] Monitoring dashboards showing all environments
- [ ] Zero manual configuration changes needed
- [ ] Rollback tested and documented
- [ ] Team trained on new process
- [ ] Documentation complete and reviewed

---

## üîó Key Tool Documentation

- **Apache NiFi**: https://nifi.apache.org/docs.html
- **NiFi Registry**: https://nifi.apache.org/docs/nifi-registry-docs/
- **Apache ZooKeeper**: https://zookeeper.apache.org/doc/
- **GitLab CI/CD**: https://docs.gitlab.com/ee/ci/
- **HashiCorp Vault**: https://www.vaultproject.io/docs
- **Prometheus**: https://prometheus.io/docs/
- **Grafana**: https://grafana.com/docs/

---

*Document Version: 1.0*  
*Last Updated: 25/11/2025*  
*Project: NiFi CI/CD Automation*  
*Owner: nifi-cicd-project Team*


